# 第1章 绪论 
1. 人工智能的意义和发展过程； 
2. 不同特征表示形式的区别以及优缺点； 
3. 神经网络的主要类型、基本结构以及在数学层面的表达。 
# 第2章：机器学习概述 
1. 常见的机器学习问题； 
2. 不同损失函数的数学公式以及适用的任务； 
3. 梯度下降法的数学意义； 
4. 什么是泛化错误，以及如何减少泛化错误； 
5. 最大似然估计和最大后验估计的异同点； 
6. 机器学习中常用评价指标的计算； 
7. 如何选择合适的机器学习模型。 
# 第3章：线性模型 
1. 交叉熵的意义以及在分类任务中的计算； 
2. KL散度的意义，了解其计算公式； 
3. argmax方式与其它分类器的区别； 
4. softmax的计算公式； 
5. 感知器的学习算法； 
6. 分类问题中常用的若干损失函数。 
# 第4章 前馈神经网络 
1. 人工神经元的结构，激活函数的性质、种类和计算； 
2. 前馈神经网络的概念与前向计算过程； 
3. 反向传播算法的计算与训练过程； 
4. 自动梯度计算的类型，了解自动微分的计算过程； 
5. 神经网络优化中的问题。 
# 第5章 卷积神经网络 
1. 卷积神经网络的概念和基本的卷积操作； 
2. 变种卷积运算的操作和其他的卷积方式； 
3. 卷积神经网络的设计和结构特性； 
4. 特征映射的概念； 
5. 典型卷积神经网络的设计，能够正确描述经典网络的特点； 
6. 卷积神经网络的应用。 
# 第6章 循环神经网络 
1. 给网络增加记忆能力的方法，掌握循环神经网络的概念； 
2. 序列机器学习任务的几种模式； 
3. 长程依赖问题及其出现的原因和缓解办法； 
4. 长短期记忆网络和门控循环单元的循环单元结构和更新方式； 
5. 深层循环神经网络的设计； 
6. 递归神经网络和图神经网络的概念； 
7. 循环神经网络应用的任务种类。 
# 第7章 网络优化与正则化 
1. 神经网络优化的难点问题； 
2. 小批量梯度下降法的思想及影响因素； 
3. 学习率调整的方法和学习率调整中的经典算法； 
4. 动量法与Adam算法； 
5. 参数初始化方法，数据预处理的作用； 
6. 各种逐层归一化的概念和它们之间的区别； 
7. 超参数优化的概念，了解优化的方法； 
8. 网络正则化中各项方法的思想与流程。 
# 第8章 注意力机制与外部记忆 
1. 注意力机制的概念和打分函数的计算方式； 
2. 注意力机制的计算过程； 
3. 键值对注意力和多头注意力的计算方式； 
4. 自注意力模型和查询-键-值模式的计算过程。 
# 第9章 无监督学习 
1. 无监督学习的概念和常见的类别； 
2. 稀疏编码的概念、训练方法与优点； 
3. 自编码器的概念，并了解自编码器的其他变形结构。 
# 第10章 模型独立的学习方式 
1. 各种学习方式的概念和基本思想，包括：集成学习、自训练、协同学习、多任务学习、迁移学习、终身学习、元学习等。 
# 第11章：概率图模型 
1. 贝叶斯网络中联合概率的计算； 
2. 局部马尔可夫性质； 
3. 有向图和无向图的关系及其转换； 
4. K-means算法的流程； 
5. EM算法。 
# 第13章：深度生成模型 
1. 变分自编码器的网络结构，了解其训练过程； 
2. GAN网络的训练过程； 
3. “再参数化”的意义； 
4. GAN模型可能存在的问题。 
# 第15章：序列生成模型 
1. 序列任务与普通任务的区别； 
2. 束搜索的算法流程； 
3. N元统计模型相关的计算； 
4. 平滑技术的意义以及计算公式； 
5. 深度序列模型常用的网络类型； 
6. BLEU和ROUGE的计算； 
7. 自注意力机制的原理； 
8. Transformer 的网络结构及各模块的意义。
